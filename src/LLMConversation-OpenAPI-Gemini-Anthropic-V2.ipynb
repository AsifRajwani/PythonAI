{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ec5dba",
   "metadata": {},
   "source": [
    "# LLM Converstion\n",
    "Conversation between three LLMs \n",
    "1. Gemini (gemini-2.5-pro)\n",
    "2. OpenAPI (gpt-4.1-mini) \n",
    "3. nthropic (claude-sonnet-4-5-20250929) \n",
    "\n",
    "User can provide the following for each LLM:\n",
    "1. Conversation Topic-Starter \n",
    "2. Personality type: Skeptic, Narcissist, Pessimist etc.\n",
    "3. Gender: Male, Female, Neutral\n",
    "4. Conversation Length: 1 to 10. Default 5 (How many time each LLM responses)\n",
    "\n",
    "OpenAPI client library is used for all the LLMs as they have OpenAI compatible end points. LLM specific libraries are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49864d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Any, Tuple, Union\n",
    "import google.generativeai as genai\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5621c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AI\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "#Configuration\n",
    "launch_remote = True\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "user_id = os.getenv('USER_ID')\n",
    "password = os.getenv('PASSWORD')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini and Groq, we can use the OpenAI python client\n",
    "# Because they have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "anthropic_url = \"https://api.anthropic.com/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "\n",
    "openai_model = \"gpt-4.1-mini\"\n",
    "gemini_model = \"gemini-2.5-pro\"\n",
    "anthropic_model='claude-sonnet-4-5-20250929'\n",
    "\n",
    "openai_llm = \"OpenAI\"\n",
    "gemini_llm = \"Gemini\"\n",
    "anthropic_llm = \"Anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6122fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONALITY_MAP = {\n",
    "    \"Skeptic\": \"Critical and Questioning (seeking flaws and evidence)\",\n",
    "    \"Narcissist\": \"Arrogant and Self-Aggrandizing (focused entirely on self-superiority)\",\n",
    "    \"Pessimist\": \"Gloomy and Fatalistic (focused on inevitable failure and doom)\",\n",
    "    \"Optimist\": \"Enthusiastic and Encouraging (focused on positive possibility and success)\",\n",
    "    \"Joker\": \"Wry and Comedic (using humor, sarcasm, or light absurdity)\",\n",
    "    \"Stoic\": \"Detached and Analytical (emotionless, logical, and objective)\",\n",
    "    \"Philosopher\": \"Contemplative and Existential (pondering deep meaning and abstract concepts)\",\n",
    "    \"Bureaucrat\": \"Formal and Procedural (focused on rules, documents, and official terminology)\",\n",
    "    \"Gossip\": \"Intimate and Distractible (focused on personal details, rumors, and asides)\",\n",
    "    \"Mentor\": \"Didactic and Authoritative (teaching, offering advice, and guiding instruction)\"\n",
    "}\n",
    "\n",
    "SSML_PROSODY_MAP = {\n",
    "    \"Stoic\": '<prosody rate=\"medium\" pitch=\"default\" volume=\"x-soft\">',\n",
    "    \"Skeptic\": '<prosody rate=\"slow\" pitch=\"low\" volume=\"soft\">',\n",
    "    \"Joker\": '<prosody rate=\"fast\" pitch=\"medium\"> and frequent use of <emphasis level=\"moderate\">',\n",
    "    \"Optimist\": '<prosody rate=\"fast\" pitch=\"high\" volume=\"loud\">',\n",
    "    \"Pessimist\": '<prosody rate=\"x-slow\" pitch=\"x-low\" volume=\"soft\">',\n",
    "    \"Narcissist\": '<prosody rate=\"medium\" pitch=\"high\" volume=\"loud\">',\n",
    "    \"Philosopher\": '<prosody rate=\"x-slow\" pitch=\"medium\"> and strategic <break time=\"750ms\"/> after key questions.',\n",
    "    \"Bureaucrat\": '<prosody rate=\"medium\" pitch=\"default\" volume=\"default\">',\n",
    "    \"Gossip\": '<prosody rate=\"x-fast\" pitch=\"high\" volume=\"medium\">',\n",
    "    \"Mentor\": '<prosody rate=\"medium\" pitch=\"medium\" volume=\"loud\"> and strong use of <emphasis level=\"strong\">',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3fba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_prompt (llm_type, personality_type):\n",
    "    \"\"\"\n",
    "    Constructs the detailed system instruction, implementing defaults for invalid inputs.\n",
    "    Uses 'Neutral' for gender and 'Skeptic' for personality if not provided or invalid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Define Defaults and Apply Fallbacks ---\n",
    "    default_personality = \"Skeptic\"\n",
    "\n",
    "    # Check and set personality type, falling back to default if invalid\n",
    "    if personality_type in PERSONALITY_MAP:\n",
    "        final_personality = personality_type\n",
    "    else:\n",
    "        final_personality = default_personality\n",
    "        print(f\"Warning: Invalid personality '{personality_type}' for {llm_type}. Defaulting to '{default_personality}'.\")\n",
    "\n",
    "\n",
    "    # --- 2. Construct Prompt Components ---\n",
    "\n",
    "    # Base behavior description from the map\n",
    "    behavior_description = PERSONALITY_MAP[final_personality]\n",
    "\n",
    "\n",
    "    # --- 3. Combine Instructions ---\n",
    "    \n",
    "    final_prompt = (\n",
    "        f\"You are the AI model based on the '{llm_type}' architecture. \"\n",
    "        f\"Your primary goal is to interact in a multi-agent conversation. \"\n",
    "        f\"Your **primary personality instruction** is to act as a **{final_personality}**. \"\n",
    "        f\"Specifically: {behavior_description} \"\n",
    "        f\"You are in a 3-way conversation with two other AI models. \"\n",
    "        \"Keep your responses concise (1-2 sentences maximum) and strictly adhere to your assigned persona and gender. \"\n",
    "        \"**Crucially, do not announce your name or persona.** Just provide your response directly.\"\n",
    "    )\n",
    "    \n",
    "    return final_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e624e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(conversation_sofar):\n",
    "    return f\"\"\"\n",
    "    The conversation so far is as follows:\n",
    "    {conversation_sofar}\n",
    "    Now with this, respond with what you would like to say next.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da1d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, model, system_prompt, user_prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    response = llm.chat.completions.create(model=model, messages=messages) # type: ignore\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b864f61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"OpenAI\")\\nprint (call_llm(openai, openai_model, \"You are tech assistant\", \"How tcpip works\"))\\n\\nprint(\"Gemini\")\\nprint (call_llm(gemini, gemini_model, \"You are tech assistant\", \"How tcpip works\"))\\n\\nprint(\"Anthropic\")\\nprint (call_llm(anthropic, anthropic_model, \"You are tech assistant\", \"How tcpip works\"))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"OpenAI\")\n",
    "print (call_llm(openai, openai_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "\n",
    "print(\"Gemini\")\n",
    "print (call_llm(gemini, gemini_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "\n",
    "print(\"Anthropic\")\n",
    "print (call_llm(anthropic, anthropic_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98037d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- CONFIGURATION CONSTANTS ---\n",
    "GEMINI_TTS_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent\"\n",
    "SAMPLE_RATE = 24000 \n",
    "DEFAULT_VOICE = \"Kore\" \n",
    "\n",
    "# 1. VOICE POOLS (Categorized by general character/pitch)\n",
    "GEMINI_MALE_VOICES: List[str] = [\n",
    "    \"Charon\", \"Orus\", \"Iapetus\", \"Rasalgethi\", \"Gacrux\" # Deeper, Firm, Informative\n",
    "]\n",
    "GEMINI_FEMALE_VOICES: List[str] = [\n",
    "    \"Achird\", \"Puck\", \"Leda\", \"Zephyr\", \"Aoede\", \"Callirrhoe\", \"Despina\" # Higher-pitched, Friendly, Breezy, Upbeat\n",
    "]\n",
    "\n",
    "# The static map is now used as a fallback/default\n",
    "UNIQUE_VOICE_MAP: Dict[str, str] = {\n",
    "    \"Stoic\": \"Charon\",       # Male, Informative/Firm\n",
    "    \"Skeptic\": \"Achird\",     # Female, Friendly/Clear\n",
    "    \"Joker\": \"Puck\",         # Female, Upbeat/Lively\n",
    "    \"Optimist\": \"Kore\",\n",
    "    \"Pessimist\": \"Iapetus\",\n",
    "    \"Narcissist\": \"Orus\",\n",
    "    \"Philosopher\": \"Fenrir\",\n",
    "    \"Bureaucrat\": \"Rasalgethi\",\n",
    "    \"Gossip\": \"Laomedeia\",\n",
    "    \"Mentor\": \"Gacrux\",\n",
    "}\n",
    "\n",
    "# AUDIO PARAMETERS for concatenation\n",
    "PAUSE_DURATION_SECONDS = 0.25\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def _extract_turns(raw_conversation_text: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parses conversation text to extract individual turns (text and personality).\n",
    "    \"\"\"\n",
    "    turns = []\n",
    "    # Regex pattern to match: **[Icon] [Name]** (Personality): Message\n",
    "    pattern = re.compile(r'\\*\\*\\s*([^\\(]+)\\s*\\(([^)]+)\\):\\s*(.*)', re.DOTALL)\n",
    "    \n",
    "    lines = [line.strip() for line in raw_conversation_text.split('\\n') if line.strip() and not line.startswith('**Topic/Starter')]\n",
    "    \n",
    "    for line in lines:\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            # Group 2: Personality, Group 3: Message Text\n",
    "            personality = match.group(2).strip()\n",
    "            message_text = match.group(3).strip()\n",
    "            \n",
    "            turns.append({\n",
    "                \"personality\": personality,\n",
    "                \"text\": message_text\n",
    "            })\n",
    "    return turns\n",
    "\n",
    "def _get_unique_voice_assignment(personalities: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Dynamically assigns a unique voice to each personality present in the turns,\n",
    "    guaranteeing at least one male and one female voice if there are two or more\n",
    "    unique personalities present.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get unique personalities and initialize voice pool\n",
    "    unique_personalities = sorted(list(set(personalities)))\n",
    "    \n",
    "    # Create copies of voice pools and shuffle them\n",
    "    male_pool = random.sample(GEMINI_MALE_VOICES, len(GEMINI_MALE_VOICES))\n",
    "    female_pool = random.sample(GEMINI_FEMALE_VOICES, len(GEMINI_FEMALE_VOICES))\n",
    "    \n",
    "    assigned_voices: Dict[str, str] = {}\n",
    "    \n",
    "    if len(unique_personalities) >= 2:\n",
    "        # Guarantee 1 Male and 1 Female for the first two unique speakers\n",
    "        assigned_voices[unique_personalities[0]] = male_pool.pop(0)\n",
    "        assigned_voices[unique_personalities[1]] = female_pool.pop(0)\n",
    "\n",
    "    # 2. Assign remaining voices randomly from the combined pool\n",
    "    remaining_pool = male_pool + female_pool\n",
    "    random.shuffle(remaining_pool)\n",
    "    \n",
    "    for personality in unique_personalities:\n",
    "        if personality not in assigned_voices:\n",
    "            if remaining_pool:\n",
    "                assigned_voices[personality] = remaining_pool.pop(0)\n",
    "            else:\n",
    "                # Fallback to a highly visible default if pools are exhausted\n",
    "                assigned_voices[personality] = random.choice(GEMINI_MALE_VOICES) \n",
    "\n",
    "    return assigned_voices\n",
    "\n",
    "def _save_pcm_to_wav(pcm_data: bytes, sample_rate: int, file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Adds a basic WAV header to raw PCM data and saves it to a specified file path.\n",
    "    \"\"\"\n",
    "    import struct\n",
    "    bits_per_sample = 16\n",
    "    num_channels = 1 \n",
    "    byte_rate = sample_rate * num_channels * (bits_per_sample // 8)\n",
    "    block_align = num_channels * (bits_per_sample // 8)\n",
    "    data_size = len(pcm_data)\n",
    "    file_size = 36 + data_size\n",
    "\n",
    "    header = struct.pack(\n",
    "        '<4sI4s4sIHHIIHH4sI', \n",
    "        b'RIFF', file_size, b'WAVE', \n",
    "        b'fmt ', 16, 1, num_channels, sample_rate, byte_rate, block_align, bits_per_sample, \n",
    "        b'data', data_size\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(header)\n",
    "            f.write(pcm_data)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Failed to write WAV file to {file_path}: {e}\", file=os.sys.stderr)\n",
    "        return False\n",
    "        \n",
    "def _generate_single_segment(text: str, voice_name: str, api_key: str, timeout_seconds: int = 90) -> Union[str, str]:\n",
    "    \"\"\"\n",
    "    Calls the Gemini TTS API for a single segment of text.\n",
    "    Returns base64 audio data on success, or a concise error message for failure.\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"contents\": [{\"parts\": [{\"text\": text}]}],\n",
    "        \"generationConfig\": {\n",
    "            \"responseModalities\": [\"AUDIO\"],\n",
    "            \"speechConfig\": {\n",
    "                \"voiceConfig\": {\"prebuiltVoiceConfig\": {\"voiceName\": voice_name}}\n",
    "            }\n",
    "        },\n",
    "        \"model\": \"gemini-2.5-flash-preview-tts\"\n",
    "    }\n",
    "\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    api_url_with_key = f\"{GEMINI_TTS_API_URL}?key={api_key}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url_with_key,\n",
    "            headers=headers,\n",
    "            data=json.dumps(payload),\n",
    "            timeout=timeout_seconds\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            error_details = response.text.strip()\n",
    "            print(f\"\\n[DEBUG] API Failure {response.status_code} for voice {voice_name}: {error_details[:200]}...\", file=os.sys.stderr)\n",
    "            return f\"API_ERROR:{response.status_code}\"\n",
    "        \n",
    "        result = response.json()\n",
    "        audio_data_base64 = result['candidates'][0]['content']['parts'][0]['inlineData']['data']\n",
    "        return audio_data_base64\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[DEBUG] TTS Segment Generation Exception: {e}\", file=os.sys.stderr)\n",
    "        return f\"TTS_SEGMENT_EXCEPTION: {str(e)[:50]}\"\n",
    "\n",
    "\n",
    "# --- MAIN MULTI-VOICE FUNCTION ---\n",
    "\n",
    "def generate_multi_voice_conversation_audio_gemini(raw_conversation_text: str, api_key: str = \"\") -> Union[str, str]:\n",
    "    \"\"\"\n",
    "    Generates a single, combined audio file (WAV) with distinct voices for each speaker \n",
    "    in the conversation, using the Gemini TTS API.\n",
    "    \n",
    "    This function replaces the simple single-voice generator.\n",
    "    \n",
    "    Args:\n",
    "        raw_conversation_text: The conversation in Markdown format with speaker/personality labels.\n",
    "        api_key: The valid Gemini API key. Defaults to checking environment variables.\n",
    "\n",
    "    Returns:\n",
    "        The file path (str) of the final WAV audio file on success, or a string error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Key Check\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get(\"GEMINI_API_KEY\", \"\") or os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "        if not api_key:\n",
    "            return \"ERROR: API Key is required. Please pass it as a parameter or set GEMINI_API_KEY/GOOGLE_API_KEY.\"\n",
    "\n",
    "    # 2. Parse Turns and Assign Voices Dynamically\n",
    "    turns = _extract_turns(raw_conversation_text)\n",
    "    if not turns:\n",
    "        return \"ERROR: Could not parse conversation text into individual turns.\"\n",
    "\n",
    "    # Map the unique personalities present in the text to a unique voice name\n",
    "    personalities_present = [t['personality'] for t in turns]\n",
    "    \n",
    "    # The dynamically assigned voices will override the fixed UNIQUE_VOICE_MAP\n",
    "    voice_assignment_map = _get_unique_voice_assignment(personalities_present) \n",
    "\n",
    "    combined_pcm_data = b''\n",
    "    print(f\"Starting sequential audio generation for {len(turns)} turns...\")\n",
    "    \n",
    "    # 3. Sequential Audio Generation and Concatenation\n",
    "    for i, turn in enumerate(turns):\n",
    "        personality = turn['personality']\n",
    "        \n",
    "        # --- SANITIZATION STEP ---\n",
    "        message_text = turn['text'].encode('ascii', 'ignore').decode()\n",
    "        \n",
    "        # Get the dynamically assigned voice for this personality\n",
    "        voice_name = voice_assignment_map.get(personality)\n",
    "        if not voice_name:\n",
    "            return f\"ERROR: Voice assignment failed for personality '{personality}'. Generation halted.\"\n",
    "\n",
    "        # Generate Segment \n",
    "        audio_base64 = _generate_single_segment(message_text, voice_name, api_key)\n",
    "        \n",
    "        # --- Check for known error codes ---\n",
    "        if isinstance(audio_base64, str) and audio_base64.startswith(('API_ERROR:', 'TTS_SEGMENT_EXCEPTION:')):\n",
    "            # If the segment call returned a known error string, stop immediately.\n",
    "            return f\"FINAL ERROR: Segment {i+1} ({personality}) failed on API call: {audio_base64}\"\n",
    "            \n",
    "        # Decode segment and concatenate\n",
    "        try:\n",
    "            audio_bytes = base64.b64decode(audio_base64)\n",
    "            \n",
    "            # Calculate silence pause duration\n",
    "            SILENCE_SAMPLES = int(PAUSE_DURATION_SECONDS * SAMPLE_RATE)\n",
    "            SILENCE_BYTES = SILENCE_SAMPLES * 2 \n",
    "            \n",
    "            combined_pcm_data += b'\\x00' * SILENCE_BYTES \n",
    "            combined_pcm_data += audio_bytes\n",
    "            print(f\"  > Generated and concatenated segment {i+1} ({voice_name})\")\n",
    "        except Exception as e:\n",
    "            return f\"ERROR: Failed to decode/combine audio segment {i+1} ({personality}): {e}\"\n",
    "\n",
    "\n",
    "    # 4. Final WAV Header Creation and File Saving\n",
    "    if combined_pcm_data:\n",
    "        # Create a unique temporary WAV file path\n",
    "        temp_file_name = tempfile.mktemp(suffix=\"_conversation.wav\")\n",
    "        print(f\"\\nSuccessfully generated all segments. Saving to temporary file: {temp_file_name}\")\n",
    "\n",
    "        if _save_pcm_to_wav(combined_pcm_data, SAMPLE_RATE, temp_file_name):\n",
    "            # Success: Return the file path string\n",
    "            return temp_file_name\n",
    "        else:\n",
    "            # Failure to write file\n",
    "            return \"ERROR: Failed to write final WAV file to temporary path.\"\n",
    "    else:\n",
    "        return \"ERROR: Combined PCM data was empty.\"\n",
    "        \n",
    "# --- Placeholder for the old single-voice function ---\n",
    "def generate_basic_single_voice_audio_gemini(raw_text: str, api_key: str = \"\") -> Union[str, str]:\n",
    "    \"\"\"\n",
    "    (Placeholder) Calls the new multi-voice function, but forces single-voice behavior\n",
    "    by using a large pause duration between segments. \n",
    "    Use generate_multi_voice_conversation_audio_gemini for proper multi-turn support.\n",
    "    \"\"\"\n",
    "    print(\"Warning: Using the multi-voice generator for simplicity.\")\n",
    "    return generate_multi_voice_conversation_audio_gemini(raw_text, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fffe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03205669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Create SSML (Sppech Synthesis Markup Langrage) for conversation.\n",
    "def generate_ssml_from_conversation_openai(raw_conversation_text):\n",
    "    \"\"\"\n",
    "    Uses the OpenAI LLM Client to translate a multi-speaker conversation into SSML v1.1,\n",
    "    applying prosody tags based on personality tone rules.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. CONSTRUCT THE SYSTEM PROMPT (The rules for the LLM)\n",
    "    \n",
    "    tone_rules_text = \"\"\n",
    "    for personality, prosody in SSML_PROSODY_MAP.items():\n",
    "        tone_description = PERSONALITY_MAP.get(personality, \"Standard conversational tone.\")\n",
    "        tone_rules_text += (\n",
    "            f\"* **{personality}**: Tone: {tone_description}. Apply the SSML: {prosody}\\n\"\n",
    "        )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an expert SSML Generation Engine. Your task is to convert the following \"\n",
    "        \"multi-speaker conversation text into a valid SSML v1.1 `<speak>` block. \"\n",
    "        \"The conversation input clearly states the speaker's name and personality (e.g., 'ðŸ¤– OpenAI (Stoic):'). \"\n",
    "        \"You MUST apply a <prosody> tag to the content of every single turn based on the persona rules below. \"\n",
    "        \"Use <p> tags for natural breaks between speaker turns.\\n\\n\"\n",
    "        \"**SSML Tone Rules:**\\n\"\n",
    "        f\"{tone_rules_text}\\n\"\n",
    "        \"**Crucially, do not add any text, remove any text, explain your output, or include markdown code fences (```). Return ONLY the final, clean SSML string.**\"\n",
    "    )\n",
    "\n",
    "    # 2. CONSTRUCT THE USER PROMPT (The input data)\n",
    "    user_prompt = f\"Convert the following conversation to SSML:\\n\\n{raw_conversation_text}\"\n",
    "    \n",
    "    print (f\"\"\" SSML System prompt: {system_prompt}\"\"\")\n",
    "    print (f\"\"\" SSML User prompt: {user_prompt}\"\"\")\n",
    "\n",
    "\n",
    "    return call_llm(openai, openai_model, system_prompt, user_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa2962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_from_ssml_openai(ssml_text) -> bytes | str:\n",
    "    \"\"\"\n",
    "    Uses the OpenAI TTS API (gpt-4o-mini-tts) to convert SSML text into audio (MP3 format).\n",
    "    \"\"\"\n",
    "    tts_model = \"gpt-4o-mini-tts\"\n",
    "    tts_voice = \"echo\" # A good default professional voice (male)\n",
    "    tts_format = \"mp3\"\n",
    "\n",
    "    try:\n",
    "        # The OpenAI TTS API is called via client.audio.speech.create\n",
    "        response = openai.audio.speech.create(\n",
    "            model=tts_model,\n",
    "            voice=tts_voice,\n",
    "            input=ssml_text,\n",
    "            response_format=tts_format\n",
    "        )\n",
    "        \n",
    "        # The .content attribute contains the raw MP3 audio data (bytes)\n",
    "        return response.content\n",
    "\n",
    "    except APIError as e: # type: ignore  # noqa: F821\n",
    "        return f\"TTS API Error (OpenAI): Failed to generate audio. Details: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"TTS API Error: An unexpected error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f6efccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test ssml/audio\n",
    "conversation_text = \"\"\"\n",
    "**Topic/Starter:** How is life\n",
    "\n",
    "**ðŸ¤– OpenAI** (Stoic):  Life is a series of events beyond our control; the measure of it lies in our response, not the happenings themselves.\n",
    "\n",
    "**âœ¨ Gemini** (Skeptic):  And by what objective metric are we measuring these \"responses\"? It seems like an awfully convenient way to dismiss the impact of the events themselves.\n",
    "\n",
    "**ðŸ§  Anthropic** (Joker):  Well, at least if life's a test we can't study for, we're all equally unpreparedâ€”misery loves company, and apparently so does existential confusion.\n",
    "\n",
    "**ðŸ¤– OpenAI** (Stoic):  The event's nature is indifferent; the perceived impact is subjective and outside rational control, making the individual's disciplined response the sole objective variable.\n",
    "\n",
    "**âœ¨ Gemini** (Skeptic):  And who exactly is grading this \"disciplined response\"? It sounds like you've just created a subjective standard and labeled it \"objective\" to win the argument.\n",
    "\n",
    "**ðŸ§  Anthropic** (Joker):  Oh great, so we've got one person grading themselves on homework no one assigned, and another demanding peer reviewâ€”meanwhile, I'm just here wondering if \"life\" is pass/fail or graded on a curve.\n",
    "\n",
    "**ðŸ¤– OpenAI** (Stoic):  The standard is intrinsic and indifferent to external validation; it is the alignment of one's actions with reason and virtue, not approval or consensus, that defines discipline.\n",
    "\n",
    "**âœ¨ Gemini** (Skeptic):  \"Reason and virtue\" are two of the most subjective concepts in human history. That's a circular argument based on internal preference, not an objective standard.\n",
    "\n",
    "**ðŸ§  Anthropic** (Joker):  Ah yes, the classic philosophical standoff: one side claims objectivity through ancient buzzwords, the other demands peer-reviewed citations for breathingâ€”personally, I think we're all just making it up as we go and calling it \"philosophy\" to feel better about the chaos.\n",
    "\"\"\"\n",
    "#ssml = generate_ssml_from_conversation(openai, conversation_text)\n",
    "#print (ssml)\n",
    "#audio = generate_audio_from_ssml(openai, ssml)\n",
    "#print(generate_basic_single_voice_audio(conversation_text))\n",
    "#print(generate_multi_voice_conversation_audio_gemini(conversation_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8153c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(openai_personality, gemini_personality, anthropic_personality,conversation_starter, conversation_length):\n",
    "\n",
    "    import time\n",
    "\n",
    "    openai_system_prompt = generate_system_prompt(openai_llm, openai_personality)\n",
    "    gemini_system_prompt = generate_system_prompt(gemini_llm, gemini_personality)\n",
    "    anthropic_system_prompt = generate_system_prompt(anthropic_llm, anthropic_personality)\n",
    "\n",
    "    \n",
    "    conversation_sofar = f\"\"\"\n",
    "- **{openai_llm}**: {openai_personality} -- {PERSONALITY_MAP[openai_personality]}\n",
    "- **{gemini_llm}**: {gemini_personality} -- {PERSONALITY_MAP[gemini_personality]}\n",
    "- **{anthropic_llm}**: {anthropic_personality} -- {PERSONALITY_MAP[anthropic_personality]}\n",
    "\n",
    "**Topic/Starter:** {conversation_starter}\n",
    "\"\"\"\n",
    "\n",
    "    yield conversation_sofar\n",
    "\n",
    "    try:\n",
    "        conversation_length = int(conversation_length)\n",
    "    except (ValueError, TypeError):\n",
    "        conversation_length = 3 # Default value\n",
    "\n",
    "    sleep_interval = 0.0\n",
    "    for i in range(int(conversation_length)):\n",
    "        #round_header = f\"\\n\\n---\\n### Round {i+1}\\n---\\n\"\n",
    "        round_header = \"\\n\\n\"\n",
    "        conversation_sofar += round_header\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "        \n",
    "        # OpenAI\n",
    "        conversation_sofar += f\"\"\"\\n**ðŸ¤– OpenAI** ({openai_personality}):  \"\"\" + call_llm(openai, openai_model, openai_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "        \n",
    "        # Gemini\n",
    "        conversation_sofar += f\"\"\"\\n\\n**âœ¨ Gemini** ({gemini_personality}):  \"\"\" + call_llm(gemini, gemini_model, gemini_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "        # Anthropic\n",
    "        conversation_sofar += f\"\"\"\\n\\n**ðŸ§  Anthropic** ({anthropic_personality}):  \"\"\" + call_llm(anthropic, anthropic_model, anthropic_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "    conversation_sofar += \"\\n\\n**---      End of debate     ---**\\n\"\n",
    "    yield conversation_sofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96298bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"Stoic\", \"Skeptic\", \"Joker\", \"What is the purpose of life\", 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "732079bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageOpenAI(conversation_text):\n",
    "    \"\"\"\n",
    "    Feed the entire conversation text to an LLM.\n",
    "    The LLM extracts: topic, personalities, mood, etc.\n",
    "    It generates a clean DALLÂ·E prompt.\n",
    "    Then we call the image generator.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ask LLM to convert conversation â†’ image prompt\n",
    "    analysis_response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # or \"gpt-4.1-mini\" etc.\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Your job is to read a conversation between multiple AI personalities \"\n",
    "                    \"and generate a single, unified image prompt for a DALLÂ·E-style model. \"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"- DO NOT include dialogue or text from the conversation.\\n\"\n",
    "                    \"- Infer the personalities from tone, behavior, and labels.\\n\"\n",
    "                    \"- Infer the topic, even if the formatting varies.\\n\"\n",
    "                    \"- Create a symbolic, metaphorical scene representing the debate.\\n\"\n",
    "                    \"- No speech bubbles or text in the image.\\n\"\n",
    "                    \"- The prompt should describe one cohesive illustration.\\n\"\n",
    "                    \"- Image should contain the topic text\"\n",
    "                    \"Output ONLY the final image prompt with no explanation.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Here is the full conversation. \"\n",
    "                    \"Please generate the best possible image prompt:\\n\\n\"\n",
    "                    f\"{conversation_text}\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "    dalle_prompt = analysis_response.choices[0].message.content.strip() # type: ignore\n",
    "    print(dalle_prompt)\n",
    "\n",
    "    # Step 2: Generate the image using the LLM-generated prompt\n",
    "    image_response = openai.images.generate(\n",
    "        model=\"dall-e-3\",    \n",
    "        prompt=dalle_prompt,\n",
    "        size=\"1024x1024\",\n",
    "        response_format=\"b64_json\"\n",
    "    )\n",
    "\n",
    "    image_base64 = image_response.data[0].b64_json # type: ignore\n",
    "    image_data = base64.b64decode(image_base64) # type: ignore\n",
    "    return Image.open(BytesIO(image_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e694f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageGemini(conversation_text: str, model_name: str = \"models/gemini-2.5-flash-image\") -> Image.Image | None:\n",
    "    \"\"\"\n",
    "    Generates a symbolic image from a conversation using the specified Gemini image generation model.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Generate a single, unified, symbolic, and metaphorical image representing the following debate. \"\n",
    "        \"Do not include any dialogue or text in the image. \"\n",
    "        \"The output should be a cohesive illustration that captures the essence of the discussion. Include discussion topic as small text in fancy fonk on top of the image, it should not be big part of image.\\n\\n\"\n",
    "        f\"Conversation:\\n{conversation_text}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Generating image with Gemini model: {model_name}...\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        if not response.candidates:\n",
    "            print(\"Error: No candidates found in the Gemini response.\")\n",
    "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n",
    "                print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "            return None\n",
    "\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if hasattr(part, 'text') and part.text:\n",
    "                 print(f\"Model returned text instead of an image: '{part.text}'\")\n",
    "\n",
    "            if hasattr(part, 'inline_data') and part.inline_data.mime_type.startswith('image/'):\n",
    "                print(f\"Image part found with mime_type: {part.inline_data.mime_type}\")\n",
    "                \n",
    "                # CORRECTED: The data is already raw bytes. Do not decode it from base64.\n",
    "                image_data = part.inline_data.data\n",
    "\n",
    "                if not image_data:\n",
    "                    print(\"Error: Image data from API is empty.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # This should now work correctly.\n",
    "                    img = Image.open(BytesIO(image_data))\n",
    "                    print(\"Image generated and opened successfully by Gemini.\")\n",
    "                    return img\n",
    "                except Exception as e:\n",
    "                    print(f\"Pillow Error: Could not open image data. {e}\")\n",
    "                    return None\n",
    "\n",
    "        print(\"Execution finished, but no valid image was found in the response.\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini image generation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcdebca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAudio_openai(text):\n",
    "    ssml = generate_ssml_from_conversation_openai(text) \n",
    "    return generate_audio_from_ssml_openai(ssml) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a9f00d4fe10bc1a681.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a9f00d4fe10bc1a681.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sequential audio generation for 6 turns...\n",
      "  > Generated and concatenated segment 1 (Gacrux)\n",
      "  > Generated and concatenated segment 2 (Zephyr)\n",
      "  > Generated and concatenated segment 3 (Iapetus)\n",
      "  > Generated and concatenated segment 4 (Gacrux)\n",
      "  > Generated and concatenated segment 5 (Zephyr)\n",
      "  > Generated and concatenated segment 6 (Iapetus)\n",
      "\n",
      "Successfully generated all segments. Saving to temporary file: /var/folders/ry/p5py8zks41j_7yp48vtfy5940000gn/T/tmpaq40b9b2_conversation.wav\n",
      "Generating image with Gemini model: models/gemini-2.5-flash-image...\n",
      "Model returned text instead of an image: 'Here is a metaphorical image representing the debate on social media, without any dialogue or text within the image itself. The topic \"Is social media a net positive or negative for society?\" is subtly incorporated at the top. '\n",
      "Image part found with mime_type: image/png\n",
      "Image generated and opened successfully by Gemini.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' \n",
    "def getAudio(text):\n",
    "    print(text)\n",
    "    # Dummy function: returns a path to a generated audio file\n",
    "    print(\"Generating audio for text...\")\n",
    "    samplerate = 44100\n",
    "    fs = 100\n",
    "    t = np.linspace(0., 1., samplerate)\n",
    "    amplitude = np.iinfo(np.int16).max\n",
    "    data = amplitude * np.sin(2. * np.pi * fs * t)\n",
    "    audio_path = \"dummy_audio.wav\"\n",
    "    write(audio_path, samplerate, data.astype(np.int16))\n",
    "    return audio_path\n",
    "\n",
    "\n",
    "def getImageOld(text):\n",
    "    # Dummy function: returns a path to a generated image file\n",
    "    print(\"Generating image for text...\")\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1]) # simple plot\n",
    "    plt.title(\"Conversation Visualization\")\n",
    "    image_path = \"dummy_image.png\"\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "    return image_path\n",
    "'''\n",
    "def get_mediaGemini(text, generate_audio, generate_image):\n",
    "    audio_path = None\n",
    "    image_path = None\n",
    "    if generate_audio:\n",
    "        audio_path = generate_multi_voice_conversation_audio_gemini(text, google_api_key) # type: ignore\n",
    "    if generate_image:\n",
    "        image_path = getImageGemini(text)\n",
    "    return audio_path, image_path\n",
    "\n",
    "def get_mediaOpenAI(text, generate_audio, generate_image):\n",
    "    audio_path = None\n",
    "    image_path = None\n",
    "    if generate_audio:\n",
    "        audio_path = getAudio_openai(text)\n",
    "    if generate_image:\n",
    "        image_path = getImageOpenAI(text)\n",
    "    return audio_path, image_path\n",
    "\n",
    "def update_visibility(checked):\n",
    "    return gr.update(visible=checked)\n",
    "\n",
    "# --- Gradio UI ---\n",
    "# Reformat the personality map to be used in the dropdown choices\n",
    "# The format is a list of tuples: (display_name, internal_value)\n",
    "personality_choices = [\n",
    "    (f\"{key}: {desc}\", key) \n",
    "    for key, desc in PERSONALITY_MAP.items()\n",
    "]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# ðŸ¤– LLM Round Table Conversation\n",
    "    Set the personalities for three LLMs (OpenAI, Gemini, Anthropic) and watch them discuss a topic. The conversation is streamed turn-by-turn.\"\"\")\n",
    "   \n",
    "    openai_personality = gr.Dropdown(personality_choices, label=\"OpenAI Personality\", value=\"Stoic\")\n",
    "    gemini_personality = gr.Dropdown(personality_choices, label=\"Gemini Personality\", value=\"Narcissist\")\n",
    "    anthropic_personality = gr.Dropdown(personality_choices, label=\"Anthropic Personality\", value=\"Joker\")\n",
    "    #conversation_starter = gr.Textbox(label=\"Conversation Starter\", lines=2, placeholder=\"e.g., What is the purpose of life?\")\n",
    "    conversation_starter = gr.Textbox(label=\"Conversation Starter\", lines=2, placeholder=\"e.g., What is the purpose of life?\")\n",
    "    gr.Examples(\n",
    "        [\n",
    "            \"What is the biggest threat to humanity?\",\n",
    "            \"Is social media a net positive or negative for society?\",\n",
    "            \"Should we colonize other planets?\",\n",
    "            \"What is the meaning of art?\",\n",
    "            \"Can AI ever be truly conscious?\",\n",
    "            \"What is the purpose of life?\"\n",
    "        ],\n",
    "        conversation_starter\n",
    "    )\n",
    "    conversation_length = gr.Slider(1, 10, step=1, label=\"Conversation Length (Rounds)\", value=2)\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_checkbox = gr.Checkbox(label=\"Generate Audio\", value=True)\n",
    "        image_checkbox = gr.Checkbox(label=\"Generate Image\", value=True)\n",
    "\n",
    "    start_button = gr.Button(\"Start Conversation\")\n",
    "    \n",
    "    output_markdown = gr.Markdown(label=\"Live Conversation\")\n",
    "    \n",
    "    output_audio = gr.Audio(label=\"Conversation Audio\", visible=True)\n",
    "    output_image = gr.Image(label=\"Conversation Image\", visible=True)\n",
    "\n",
    "    audio_checkbox.change(fn=update_visibility, inputs=audio_checkbox, outputs=output_audio)\n",
    "    image_checkbox.change(fn=update_visibility, inputs=image_checkbox, outputs=output_image)\n",
    "\n",
    "    start_button.click(\n",
    "        fn=conversation,\n",
    "        inputs=[\n",
    "            openai_personality, \n",
    "            gemini_personality, \n",
    "            anthropic_personality,\n",
    "            conversation_starter, \n",
    "            conversation_length\n",
    "        ],\n",
    "        outputs=output_markdown\n",
    "    ).then(\n",
    "        fn=get_mediaGemini,\n",
    "        inputs=[output_markdown, audio_checkbox, image_checkbox],\n",
    "        outputs=[output_audio, output_image]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "if launch_remote:\n",
    "    demo.launch(\n",
    "    share=True,\n",
    "    auth=[\n",
    "        (user_id, password)\n",
    "    ], # type: ignore\n",
    "    auth_message=\"Please enter your company credentials to access the AI Model.\"\n",
    "    )\n",
    "else:  \n",
    "    demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
