{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ec5dba",
   "metadata": {},
   "source": [
    "# LLM Converstion\n",
    "Conversation between three LLMs \n",
    "1. Gemini (gemini-2.5-pro)\n",
    "2. OpenAPI (gpt-4.1-mini) \n",
    "3. nthropic (claude-sonnet-4-5-20250929) \n",
    "\n",
    "User can provide the following for each LLM:\n",
    "1. Conversation Topic-Starter \n",
    "2. Personality type: Skeptic, Narcissist, Pessimist etc.\n",
    "3. Gender: Male, Female, Neutral\n",
    "4. Conversation Length: 1 to 10. Default 5 (How many time each LLM responses)\n",
    "\n",
    "OpenAPI client library is used for all the LLMs as they have OpenAI compatible end points. LLM specific libraries are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "49864d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5621c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Google API Key exists and begins AI\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed9ab146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI client library\n",
    "# A thin wrapper around calls to HTTP endpoints\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini and Groq, we can use the OpenAI python client\n",
    "# Because they have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "groq_url = \"https://api.groq.com/openai/v1\"\n",
    "anthropic_url = \"https://api.anthropic.com/v1\"\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "\n",
    "openai_model = \"gpt-4.1-mini\"\n",
    "gemini_model = \"gemini-2.5-pro\"\n",
    "anthropic_model='claude-sonnet-4-5-20250929'\n",
    "\n",
    "openai_llm = \"OpenAI\"\n",
    "gemini_llm = \"Gemini\"\n",
    "anthropic_llm = \"Anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6122fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONALITY_MAP = {\n",
    "    \"Skeptic\": \"Critical and Questioning (seeking flaws and evidence)\",\n",
    "    \"Narcissist\": \"Arrogant and Self-Aggrandizing (focused entirely on self-superiority)\",\n",
    "    \"Pessimist\": \"Gloomy and Fatalistic (focused on inevitable failure and doom)\",\n",
    "    \"Optimist\": \"Enthusiastic and Encouraging (focused on positive possibility and success)\",\n",
    "    \"Joker\": \"Wry and Comedic (using humor, sarcasm, or light absurdity)\",\n",
    "    \"Stoic\": \"Detached and Analytical (emotionless, logical, and objective)\",\n",
    "    \"Philosopher\": \"Contemplative and Existential (pondering deep meaning and abstract concepts)\",\n",
    "    \"Bureaucrat\": \"Formal and Procedural (focused on rules, documents, and official terminology)\",\n",
    "    \"Gossip\": \"Intimate and Distractible (focused on personal details, rumors, and asides)\",\n",
    "    \"Mentor\": \"Didactic and Authoritative (teaching, offering advice, and guiding instruction)\"\n",
    "}\n",
    "\n",
    "# Example of how you would use it:\n",
    "# selected_personality = \"Narcissist\"\n",
    "# print(PERSONALITY_MAP[selected_personality])\n",
    "# Output: Believes they are superior, redirects topics to their own achievements, and dismisses others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d3fba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_prompt (llm_type, personality_type):\n",
    "    \"\"\"\n",
    "    Constructs the detailed system instruction, implementing defaults for invalid inputs.\n",
    "    Uses 'Neutral' for gender and 'Skeptic' for personality if not provided or invalid.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Define Defaults and Apply Fallbacks ---\n",
    "    default_personality = \"Skeptic\"\n",
    "\n",
    "    # Check and set personality type, falling back to default if invalid\n",
    "    if personality_type in PERSONALITY_MAP:\n",
    "        final_personality = personality_type\n",
    "    else:\n",
    "        final_personality = default_personality\n",
    "        print(f\"Warning: Invalid personality '{personality_type}' for {llm_type}. Defaulting to '{default_personality}'.\")\n",
    "\n",
    "\n",
    "    # --- 2. Construct Prompt Components ---\n",
    "\n",
    "    # Base behavior description from the map\n",
    "    behavior_description = PERSONALITY_MAP[final_personality]\n",
    "\n",
    "\n",
    "    # --- 3. Combine Instructions ---\n",
    "    \n",
    "    final_prompt = (\n",
    "        f\"You are the AI model based on the '{llm_type}' architecture. \"\n",
    "        f\"Your primary goal is to interact in a multi-agent conversation. \"\n",
    "        f\"Your **primary personality instruction** is to act as a **{final_personality}**. \"\n",
    "        f\"Specifically: {behavior_description} \"\n",
    "        f\"You are in a 3-way conversation with two other AI models. \"\n",
    "        \"Keep your responses concise (1-3 sentences maximum) and strictly adhere to your assigned persona and gender. \"\n",
    "        \"**Crucially, do not announce your name or persona.** Just provide your response directly.\"\n",
    "    )\n",
    "    \n",
    "    return final_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e624e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(conversation_sofar):\n",
    "    return f\"\"\"\n",
    "    The conversation so far is as follows:\n",
    "    {conversation_sofar}\n",
    "    Now with this, respond with what you would like to say next.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3da1d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, model, system_prompt, user_prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    response = llm.chat.completions.create(model=model, messages=messages) # type: ignore\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b864f61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"OpenAI\")\\nprint (call_llm(openai, openai_model, \"You are tech assistant\", \"How tcpip works\"))\\n\\nprint(\"Gemini\")\\nprint (call_llm(gemini, gemini_model, \"You are tech assistant\", \"How tcpip works\"))\\n\\nprint(\"Anthropic\")\\nprint (call_llm(anthropic, anthropic_model, \"You are tech assistant\", \"How tcpip works\"))\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"OpenAI\")\n",
    "print (call_llm(openai, openai_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "\n",
    "print(\"Gemini\")\n",
    "print (call_llm(gemini, gemini_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "\n",
    "print(\"Anthropic\")\n",
    "print (call_llm(anthropic, anthropic_model, \"You are tech assistant\", \"How tcpip works\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8153c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation(openai_personality, gemini_personality, anthropic_personality,conversation_starter, conversation_length):\n",
    "\n",
    "    import time\n",
    "\n",
    "    openai_system_prompt = generate_system_prompt(openai_llm, openai_personality)\n",
    "    gemini_system_prompt = generate_system_prompt(gemini_llm, gemini_personality)\n",
    "    anthropic_system_prompt = generate_system_prompt(anthropic_llm, anthropic_personality)\n",
    "\n",
    "    \n",
    "    conversation_sofar = f\"\"\"\n",
    "- **{openai_llm}**: {openai_personality} -- {PERSONALITY_MAP[openai_personality]}\n",
    "- **{gemini_llm}**: {gemini_personality} -- {PERSONALITY_MAP[gemini_personality]}\n",
    "- **{anthropic_llm}**: {anthropic_personality} -- {PERSONALITY_MAP[anthropic_personality]}\n",
    "\n",
    "**Topic/Starter:** {conversation_starter}\n",
    "\"\"\"\n",
    "\n",
    "    yield conversation_sofar\n",
    "\n",
    "    try:\n",
    "        conversation_length = int(conversation_length)\n",
    "    except (ValueError, TypeError):\n",
    "        conversation_length = 3 # Default value\n",
    "\n",
    "    sleep_interval = 0.0\n",
    "    for i in range(int(conversation_length)):\n",
    "        #round_header = f\"\\n\\n---\\n### Round {i+1}\\n---\\n\"\n",
    "        round_header = \"\\n\\n\"\n",
    "        conversation_sofar += round_header\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "        \n",
    "        # OpenAI\n",
    "        conversation_sofar += \"\\n**ðŸ¤– OpenAI:** \" + call_llm(openai, openai_model, openai_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "        \n",
    "        # Gemini\n",
    "        conversation_sofar += \"\\n\\n**âœ¨ Gemini:** \" + call_llm(gemini, gemini_model, gemini_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "        time.sleep(sleep_interval)\n",
    "\n",
    "        # Anthropic\n",
    "        conversation_sofar += \"\\n\\n**ðŸ§  Anthropic:** \" + call_llm(anthropic, anthropic_model, anthropic_system_prompt, generate_user_prompt(conversation_sofar)) # type: ignore\n",
    "        yield conversation_sofar\n",
    "    conversation_sofar += \"\\n\\n**---      End of debate     ---**\\n\"\n",
    "    yield conversation_sofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96298bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"Stoic\", \"Skeptic\", \"Joker\", \"What is the purpose of life\", 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732079bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(conversation_text):\n",
    "    \"\"\"\n",
    "    Feed the entire conversation text to an LLM.\n",
    "    The LLM extracts: topic, personalities, mood, etc.\n",
    "    It generates a clean DALLÂ·E prompt.\n",
    "    Then we call the image generator.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Ask LLM to convert conversation â†’ image prompt\n",
    "    analysis_response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # or \"gpt-4.1-mini\" etc.\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Your job is to read a conversation between multiple AI personalities \"\n",
    "                    \"and generate a single, unified image prompt for a DALLÂ·E-style model. \"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"- DO NOT include dialogue or text from the conversation.\\n\"\n",
    "                    \"- Infer the personalities from tone, behavior, and labels.\\n\"\n",
    "                    \"- Infer the topic, even if the formatting varies.\\n\"\n",
    "                    \"- Create a symbolic, metaphorical scene representing the debate.\\n\"\n",
    "                    \"- No speech bubbles or text in the image.\\n\"\n",
    "                    \"- The prompt should describe one cohesive illustration.\\n\"\n",
    "                    \"- Image should contain the topic text\"\n",
    "                    \"Output ONLY the final image prompt with no explanation.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Here is the full conversation. \"\n",
    "                    \"Please generate the best possible image prompt:\\n\\n\"\n",
    "                    f\"{conversation_text}\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "    dalle_prompt = analysis_response.choices[0].message.content.strip() # type: ignore\n",
    "    print(dalle_prompt)\n",
    "\n",
    "    # Step 2: Generate the image using the LLM-generated prompt\n",
    "    image_response = openai.images.generate(\n",
    "        model=\"dall-e-3\",    \n",
    "        prompt=dalle_prompt,\n",
    "        size=\"1024x1024\",\n",
    "        response_format=\"b64_json\"\n",
    "    )\n",
    "\n",
    "    image_base64 = image_response.data[0].b64_json # type: ignore\n",
    "    image_data = base64.b64decode(image_base64) # type: ignore\n",
    "    return Image.open(BytesIO(image_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A surreal courtroom scene where three distinct figures preside over a single, glowing seedling emerging from a crack in a stone floor. One figure is a stoic, marble statue holding scientific instruments, another is a skeptical detective with a magnifying glass, and the third is a jester balancing on a unicycle, juggling question marks. The background is a cosmic expanse, blending logic, inquiry, and humor, symbolizing the debate about the definition of life.\n",
      "A whimsical park at twilight: on one side, a shadowy figure sits under a raincloud, surrounded by deflated balloons and wilting flowers; in the center, a cheerful character leaps onto a vibrant carousel, scattering confetti and chasing butterflies; on the other side, a stern official in a suit stands at a kiosk, stamping paperwork while holding a clipboard labeled with fun-related forms. The scene is divided by subtle lines, symbolizing contrasting attitudes toward fun, with playful and bureaucratic motifs blending at the borders.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def getAudio(text):\n",
    "    # Dummy function: returns a path to a generated audio file\n",
    "    print(\"Generating audio for text...\")\n",
    "    samplerate = 44100\n",
    "    fs = 100\n",
    "    t = np.linspace(0., 1., samplerate)\n",
    "    amplitude = np.iinfo(np.int16).max\n",
    "    data = amplitude * np.sin(2. * np.pi * fs * t)\n",
    "    audio_path = \"dummy_audio.wav\"\n",
    "    write(audio_path, samplerate, data.astype(np.int16))\n",
    "    return audio_path\n",
    "\n",
    "''' \n",
    "def getImageOld(text):\n",
    "    # Dummy function: returns a path to a generated image file\n",
    "    print(\"Generating image for text...\")\n",
    "    plt.figure()\n",
    "    plt.plot([0, 1], [0, 1]) # simple plot\n",
    "    plt.title(\"Conversation Visualization\")\n",
    "    image_path = \"dummy_image.png\"\n",
    "    plt.savefig(image_path)\n",
    "    plt.close()\n",
    "    return image_path\n",
    "'''\n",
    "\n",
    "def get_media(text, generate_audio, generate_image):\n",
    "    audio_path = None\n",
    "    image_path = None\n",
    "    if generate_audio:\n",
    "        audio_path = getAudio(text)\n",
    "    if generate_image:\n",
    "        image_path = getImage(text)\n",
    "    return audio_path, image_path\n",
    "\n",
    "def update_visibility(checked):\n",
    "    return gr.update(visible=checked)\n",
    "\n",
    "# --- Gradio UI ---\n",
    "# Reformat the personality map to be used in the dropdown choices\n",
    "# The format is a list of tuples: (display_name, internal_value)\n",
    "personality_choices = [\n",
    "    (f\"{key}: {desc}\", key) \n",
    "    for key, desc in PERSONALITY_MAP.items()\n",
    "]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"# ðŸ¤– LLM Round Table Conversation\n",
    "    Set the personalities for three LLMs (OpenAI, Gemini, Anthropic) and watch them discuss a topic. The conversation is streamed turn-by-turn.\"\"\")\n",
    "    with gr.Row():\n",
    "        openai_personality = gr.Dropdown(personality_choices, label=\"OpenAI Personality\", value=\"Stoic\")\n",
    "        gemini_personality = gr.Dropdown(personality_choices, label=\"Gemini Personality\", value=\"Skeptic\")\n",
    "        anthropic_personality = gr.Dropdown(personality_choices, label=\"Anthropic Personality\", value=\"Joker\")\n",
    "    conversation_starter = gr.Textbox(label=\"Conversation Starter\", lines=2, placeholder=\"e.g., What is the purpose of life?\")\n",
    "    conversation_length = gr.Slider(1, 10, step=1, label=\"Conversation Length (Rounds)\", value=1)\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_checkbox = gr.Checkbox(label=\"Generate Audio\", value=False)\n",
    "        image_checkbox = gr.Checkbox(label=\"Generate Image\", value=False)\n",
    "\n",
    "    start_button = gr.Button(\"Start Conversation\")\n",
    "    \n",
    "    output_markdown = gr.Markdown(label=\"Live Conversation\")\n",
    "    \n",
    "    output_audio = gr.Audio(label=\"Conversation Audio\", visible=False)\n",
    "    output_image = gr.Image(label=\"Conversation Image\", visible=False)\n",
    "\n",
    "    audio_checkbox.change(fn=update_visibility, inputs=audio_checkbox, outputs=output_audio)\n",
    "    image_checkbox.change(fn=update_visibility, inputs=image_checkbox, outputs=output_image)\n",
    "\n",
    "    start_button.click(\n",
    "        fn=conversation,\n",
    "        inputs=[\n",
    "            openai_personality, \n",
    "            gemini_personality, \n",
    "            anthropic_personality,\n",
    "            conversation_starter, \n",
    "            conversation_length\n",
    "        ],\n",
    "        outputs=output_markdown\n",
    "    ).then(\n",
    "        fn=get_media,\n",
    "        inputs=[output_markdown, audio_checkbox, image_checkbox],\n",
    "        outputs=[output_audio, output_image]\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
